{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68118e70-a574-43c1-9d9b-b16a00dce2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8caac6-8494-477a-9481-bbcf0119b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "199\n",
      "Alles Gut. Das Trennzeichen ist in tokens enthalten.\n",
      "Alles Gut.\n",
      "Alles Gut.\n"
     ]
    }
   ],
   "source": [
    "text4000 = pd.read_csv(\".//Data/4000/4000-stories-VAD.csv\")\n",
    "texts= list(text4000.story.values)\n",
    "# Texte mit Trennzeichen oder speziellem Wort kombinieren\n",
    "trennzeichen = \"trennzeichen\"\n",
    "joined_text = (\" \" + trennzeichen + \" \").join(texts)\n",
    "# Tokenizer erstellen und Texte darauf anwenden\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(joined_text.lower())\n",
    "context_words = 10\n",
    "input_words = []\n",
    "next_words = []\n",
    "words_limiter = 100000 # limitiert die Anzahl an Trainingsdaten\n",
    "counter=0\n",
    "i = 0\n",
    "while i < len(tokens) - context_words:\n",
    "    if tokens[i + context_words] == trennzeichen:\n",
    "        i += context_words + 1\n",
    "        counter+=1\n",
    "        continue  # Eintrag überspringen, wenn das Trennzeichen erreicht wird\n",
    "    \n",
    "    input_words.append(tokens[i:i + context_words])\n",
    "    next_words.append(tokens[i + context_words])\n",
    "    \n",
    "    if len(next_words) >= words_limiter:\n",
    "        break\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(len(next_words))\n",
    "print(counter)\n",
    "# überprüfen ob alles ok mit dem Trennzeichen ist\n",
    "if trennzeichen in tokens:\n",
    "    print(\"Alles Gut. Das Trennzeichen ist in tokens enthalten.\")\n",
    "else:\n",
    "    print(\"Fehler!!! Trennzeichen nicht in tokens!!!\")\n",
    "if trennzeichen in input_words:\n",
    "    print(\"Fehler!!! Das Trennzeichen ist in input_words enthalten.!!!\")\n",
    "else:\n",
    "    print(\"Alles Gut.\")\n",
    "if trennzeichen in next_words:\n",
    "    print(\"Fehler!!! Das Trennzeichen ist in next_words enthalten.!!!\")\n",
    "else:\n",
    "    print(\"Alles Gut.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327b54d7-1aed-482c-8271-ec4fe76b6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere input_words in einen eindimensionalen Array von Strings\n",
    "input_words_flat = np.concatenate(input_words).ravel()\n",
    "\n",
    "# Kombiniere input_words_flat mit next_words\n",
    "combined_array = np.concatenate((input_words_flat, next_words))\n",
    "\n",
    "# Verwandle den kombinierten Array in einen einzelnen langen String\n",
    "combined_string = ' '.join(combined_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07923db-11f7-4526-bfc6-60b6e8a5f007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14353\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([combined_string])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "total_unique_words = len(tokenizer.word_index) + 1\n",
    "print(total_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bdabc1a-ad0b-4ef3-8d47-4ffd87e5249e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(input_words)\n\u001b[1;32m----> 2\u001b[0m next_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts_to_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_words:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_words[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Beispiel für die Umwandlung des zweiten input_words\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\preprocessing\\text.py:357\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtexts_to_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;124;03m\"\"\"Transforms each text in texts to a sequence of integers.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m        A list of sequences.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts_to_sequences_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\preprocessing\\text.py:386\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_word_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer(text)\n",
      "File \u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\preprocessing\\text.py:78\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     76\u001b[0m translate_dict \u001b[38;5;241m=\u001b[39m {c: split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m filters}\n\u001b[0;32m     77\u001b[0m translate_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(translate_dict)\n\u001b[1;32m---> 78\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[43minput_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslate_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m seq \u001b[38;5;241m=\u001b[39m input_text\u001b[38;5;241m.\u001b[39msplit(split)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m seq \u001b[38;5;28;01mif\u001b[39;00m i]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_sequences = tokenizer.texts_to_sequences(input_words)\n",
    "next_sequences = tokenizer.texts_to_sequences(next_words)\n",
    "\n",
    "print(\"input_words:\")\n",
    "print(input_words[1])  # Beispiel für die Umwandlung des zweiten input_words\n",
    "\n",
    "print(\"next_word:\")\n",
    "print(next_words[1])\n",
    "\n",
    "print(\"Input Sequences:\")\n",
    "print(input_sequences[1])  # Beispiel für die Umwandlung des zweiten input_words\n",
    "\n",
    "print(\"Next Sequences:\")\n",
    "print(next_sequences[1])  # Beispiel für die Umwandlung des zweiten target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f1fba-e92e-408f-a197-66ca86346315",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # Dimension der GloVe-Vektoren\n",
    "embeddings_index = {}  # Dictionary für die GloVe-Vektoren\n",
    "\n",
    "# Laden der GloVe-Daten\n",
    "path = 'glove.6B/glove.6B.100d.txt'\n",
    "with open(path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.array(values[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = coeffs\n",
    "\n",
    "# Erstellen der embeddings_matrix\n",
    "num_words = min(words_limiter, total_unique_words)  # Anzahl der eindeutigen Tokens, die verwendet werden\n",
    "embeddings_matrix = np.zeros((num_words, embedding_dim))  # Initialisierung der Matrix mit Nullen\n",
    "\n",
    "counterIN=0\n",
    "counterOut=0\n",
    "for word, i in word_index.items():\n",
    "    if i >= words_limiter:\n",
    "        continue  # Nur die ersten words_limiter eindeutigen Tokens verwenden\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector\n",
    "        counterIN+=1\n",
    "    else:\n",
    "        # Wenn das Wort nicht in den GloVe-Vektoren vorhanden ist, wird es mit zufälligen Werten initialisiert\n",
    "        embeddings_matrix[i] = np.random.uniform(-0.25, 0.25, embedding_dim)\n",
    "        counterOut+=1\n",
    "print(counterIN)\n",
    "print(counterOut)\n",
    "# embeddings_matrix = np.zeros((total_unique_words, 100))\n",
    "# for word, i in word_index.items():\n",
    "#    embedding_vector = embeddings_index.get(word)\n",
    "#    if embedding_vector is not None:\n",
    "#      embeddings_matrix[i] = embedding_vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f728cb-dadf-4a0e-912d-7ad427c0d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(input_sequences) * 0.95)\n",
    "x_train, x_val = input_sequences[:split_index], input_sequences[split_index:]\n",
    "y_train, y_val = next_sequences[:split_index], next_sequences[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2fe20-a7c3-444e-9df7-8583d1f152e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(x_val[0])\n",
    "print(y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e00f0b-e4a9-493f-8273-18ae443f0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FormatedData/E100000/x_train.pickle', 'wb') as file:\n",
    "    pickle.dump(x_train, file)\n",
    "with open('FormatedData/E100000/x_val.pickle', 'wb') as file:\n",
    "    pickle.dump(x_val, file)\n",
    "with open('FormatedData/E100000/y_train.pickle', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "with open('FormatedData/E100000/y_val.pickle', 'wb') as file:\n",
    "    pickle.dump(y_val, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30f9f2-9bde-4577-82f0-560b73a9e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FormatedData/E100000/MetaData/context_words.pickle', 'wb') as file:\n",
    "    pickle.dump(context_words, file)\n",
    "# with open('FormatedData/E100000/MetaData/unique_tokens.pickle', 'wb') as file:\n",
    "#     pickle.dump(unique_tokens, file)\n",
    "with open('FormatedData/E100000/MetaData/unique_token_index.pickle', 'wb') as file:\n",
    "    pickle.dump(word_index, file)\n",
    "with open('FormatedData/E100000/MetaData/total_unique_words.pickle', 'wb') as file:\n",
    "    pickle.dump(total_unique_words, file) \n",
    "with open('FormatedData/E100000/MetaData/embeddings_matrix.pickle', 'wb') as file:\n",
    "    pickle.dump(embeddings_matrix, file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a763c-1eee-47e0-afa3-7bc0c5d56d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FormatedData/E100000/RawData/input_words.pickle', 'wb') as file:\n",
    "    pickle.dump(input_words, file)\n",
    "with open('FormatedData/E100000/RawData/next_words.pickle', 'wb') as file:\n",
    "    pickle.dump(next_words, file)\n",
    "with open('FormatedData/E100000/RawData/story_counter.pickle', 'wb') as file:\n",
    "    pickle.dump(counter, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba990a73-8979-4a82-83db-069b99c7ad63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
