{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68118e70-a574-43c1-9d9b-b16a00dce2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b8caac6-8494-477a-9481-bbcf0119b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "20\n",
      "Alles Gut. Das Trennzeichen ist in tokens enthalten.\n",
      "Alles Gut.\n",
      "Alles Gut.\n"
     ]
    }
   ],
   "source": [
    "text4000 = pd.read_csv(\".//Data/4000/4000-stories-VAD.csv\")\n",
    "texts= list(text4000.story.values)\n",
    "# Texte mit Trennzeichen oder speziellem Wort kombinieren\n",
    "trennzeichen = \"trennzeichen\"\n",
    "joined_text = (\" \" + trennzeichen + \" \").join(texts)\n",
    "# Tokenizer erstellen und Texte darauf anwenden\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(joined_text.lower())\n",
    "context_words = 10\n",
    "input_words = []\n",
    "next_words = []\n",
    "words_limiter = 100000 # limitiert die Anzahl an Trainingsdaten\n",
    "counter=0\n",
    "i = 0\n",
    "while i < len(tokens) - context_words:\n",
    "    if tokens[i + context_words] == trennzeichen:\n",
    "        i += context_words + 1\n",
    "        counter+=1\n",
    "        continue  # Eintrag überspringen, wenn das Trennzeichen erreicht wird\n",
    "    \n",
    "    input_words.append(tokens[i:i + context_words])\n",
    "    next_words.append(tokens[i + context_words])\n",
    "    \n",
    "    if len(next_words) >= words_limiter:\n",
    "        break\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(len(next_words))\n",
    "print(counter)\n",
    "# überprüfen ob alles ok mit dem Trennzeichen ist\n",
    "if trennzeichen in tokens:\n",
    "    print(\"Alles Gut. Das Trennzeichen ist in tokens enthalten.\")\n",
    "else:\n",
    "    print(\"Fehler!!! Trennzeichen nicht in tokens!!!\")\n",
    "if trennzeichen in input_words:\n",
    "    print(\"Fehler!!! Das Trennzeichen ist in input_words enthalten.!!!\")\n",
    "else:\n",
    "    print(\"Alles Gut.\")\n",
    "if trennzeichen in next_words:\n",
    "    print(\"Fehler!!! Das Trennzeichen ist in next_words enthalten.!!!\")\n",
    "else:\n",
    "    print(\"Alles Gut.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327b54d7-1aed-482c-8271-ec4fe76b6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere input_words in einen eindimensionalen Array von Strings\n",
    "input_words_flat = np.concatenate(input_words).ravel()\n",
    "\n",
    "# Kombiniere input_words_flat mit next_words\n",
    "combined_array = np.concatenate((input_words_flat, next_words))\n",
    "\n",
    "# Verwandle den kombinierten Array in einen einzelnen langen String\n",
    "combined_string = ' '.join(combined_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f07923db-11f7-4526-bfc6-60b6e8a5f007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9808\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([combined_string])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "total_unique_words = len(tokenizer.word_index) + 1\n",
    "print(total_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bdabc1a-ad0b-4ef3-8d47-4ffd87e5249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_words:\n",
      "['i', 'if', 'you', 'don', 't', 'like', 'christmas', 'stories', 'don', 't']\n",
      "next_word:\n",
      "read\n",
      "Input Sequences:\n",
      "[10, 51, 13, 72, 29, 53, 279, 2166, 72, 29]\n",
      "Next Sequences:\n",
      "[526]\n"
     ]
    }
   ],
   "source": [
    "input_sequences = tokenizer.texts_to_sequences(input_words)\n",
    "next_sequences = tokenizer.texts_to_sequences(next_words)\n",
    "\n",
    "print(\"input_words:\")\n",
    "print(input_words[1])  # Beispiel für die Umwandlung des zweiten input_words\n",
    "\n",
    "print(\"next_word:\")\n",
    "print(next_words[1])\n",
    "\n",
    "print(\"Input Sequences:\")\n",
    "print(input_sequences[1])  # Beispiel für die Umwandlung des zweiten input_words\n",
    "\n",
    "print(\"Next Sequences:\")\n",
    "print(next_sequences[1])  # Beispiel für die Umwandlung des zweiten target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "016f1fba-e92e-408f-a197-66ca86346315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9480\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100  # Dimension der GloVe-Vektoren\n",
    "embeddings_index = {}  # Dictionary für die GloVe-Vektoren\n",
    "\n",
    "# Laden der GloVe-Daten\n",
    "path = 'glove.6B/glove.6B.100d.txt'\n",
    "with open(path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.array(values[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = coeffs\n",
    "\n",
    "# Erstellen der embeddings_matrix\n",
    "num_words = min(words_limiter, total_unique_words)  # Anzahl der eindeutigen Tokens, die verwendet werden\n",
    "embeddings_matrix = np.zeros((num_words, embedding_dim))  # Initialisierung der Matrix mit Nullen\n",
    "\n",
    "counterIN=0\n",
    "counterOut=0\n",
    "for word, i in word_index.items():\n",
    "    if i >= words_limiter:\n",
    "        continue  # Nur die ersten words_limiter eindeutigen Tokens verwenden\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector\n",
    "        counterIN+=1\n",
    "    else:\n",
    "        # Wenn das Wort nicht in den GloVe-Vektoren vorhanden ist, wird es mit zufälligen Werten initialisiert\n",
    "        embeddings_matrix[i] = np.random.uniform(-0.25, 0.25, embedding_dim)\n",
    "        counterOut+=1\n",
    "print(counterIN)\n",
    "print(counterOut)\n",
    "# embeddings_matrix = np.zeros((total_unique_words, 100))\n",
    "# for word, i in word_index.items():\n",
    "#    embedding_vector = embeddings_index.get(word)\n",
    "#    if embedding_vector is not None:\n",
    "#      embeddings_matrix[i] = embedding_vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f728cb-dadf-4a0e-912d-7ad427c0d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(input_sequences) * 0.95)\n",
    "x_train, x_val = input_sequences[:split_index], input_sequences[split_index:]\n",
    "y_train, y_val = next_sequences[:split_index], next_sequences[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf2fe20-a7c3-444e-9df7-8583d1f152e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[452, 10, 51, 13, 72, 29, 53, 279, 2166, 72]\n",
      "[29]\n",
      "[1, 2163, 7, 667, 2, 82, 323, 269, 1103, 97]\n",
      "[142]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(x_val[0])\n",
    "print(y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e00f0b-e4a9-493f-8273-18ae443f0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FormatedData/E100000/x_train.pickle', 'wb') as file:\n",
    "    pickle.dump(x_train, file)\n",
    "with open('FormatedData/E100000/x_val.pickle', 'wb') as file:\n",
    "    pickle.dump(x_val, file)\n",
    "with open('FormatedData/E100000/y_train.pickle', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "with open('FormatedData/E100000/y_val.pickle', 'wb') as file:\n",
    "    pickle.dump(y_val, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a30f9f2-9bde-4577-82f0-560b73a9e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FormatedData/E100000/MetaData/context_words.pickle', 'wb') as file:\n",
    "    pickle.dump(context_words, file)\n",
    "# with open('FormatedData/E100000/MetaData/unique_tokens.pickle', 'wb') as file:\n",
    "#     pickle.dump(unique_tokens, file)\n",
    "with open('FormatedData/E100000/MetaData/unique_token_index.pickle', 'wb') as file:\n",
    "    pickle.dump(word_index, file)\n",
    "with open('FormatedData/E100000/MetaData/total_unique_words.pickle', 'wb') as file:\n",
    "    pickle.dump(total_unique_words, file) \n",
    "with open('FormatedData/E100000/MetaData/embeddings_matrix.pickle', 'wb') as file:\n",
    "    pickle.dump(embeddings_matrix, file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08a763c-1eee-47e0-afa3-7bc0c5d56d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FormatedData/E100000/RawData/input_words.pickle', 'wb') as file:\n",
    "    pickle.dump(input_words, file)\n",
    "with open('FormatedData/E100000/RawData/next_words.pickle', 'wb') as file:\n",
    "    pickle.dump(next_words, file)\n",
    "with open('FormatedData/E100000/RawData/story_counter.pickle', 'wb') as file:\n",
    "    pickle.dump(counter, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba990a73-8979-4a82-83db-069b99c7ad63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
